{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Z0HiNpYt6zD"
   },
   "source": [
    "# Big Earth Net Preprocessing\n",
    "## Irrigation Capstone Fall 2020\n",
    "### TP Goter\n",
    "\n",
    "This notebook is used to preprocess the GeoTiff files that contain the Sentinel-2 MSI data comprising the BigEarthNet dataset into TFRecords files. It is based on the preprocessing scripts from the BigEarthNet repo, but has been updated to work in Colaboratory with Python3.7+ and TensorFlow 2.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4841,
     "status": "ok",
     "timestamp": 1601156548878,
     "user": {
      "displayName": "Thomas Goter",
      "photoUrl": "",
      "userId": "00883949598941594885"
     },
     "user_tz": 240
    },
    "id": "b0r4VAo9eRWa",
    "outputId": "0d884a87-7e43-470a-98f9-0312431f02ba"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4836,
     "status": "ok",
     "timestamp": 1601156548880,
     "user": {
      "displayName": "Thomas Goter",
      "photoUrl": "",
      "userId": "00883949598941594885"
     },
     "user_tz": 240
    },
    "id": "i8AXp5QRfCMR",
    "outputId": "58c67cd6-4e0a-452b-ba6b-2414f9a44ba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n",
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(pd.__version__)\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFjjrUFAucPm"
   },
   "source": [
    "## Mount Google Drive and Set Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4830,
     "status": "ok",
     "timestamp": 1601156548880,
     "user": {
      "displayName": "Thomas Goter",
      "photoUrl": "",
      "userId": "00883949598941594885"
     },
     "user_tz": 240
    },
    "id": "Q5hp7SNtfCvY",
    "outputId": "3db919a6-5cd3-4949-8f0a-0f0e03e62145"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nvKWoMlfGZ4"
   },
   "outputs": [],
   "source": [
    "base_path = '/content/gdrive/My Drive/Capstone Project'\n",
    "big_earth_path ='./BigEarthNet-v1.0/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUcdgz7bugBq"
   },
   "source": [
    "## Create Symbolic Link(s)\n",
    "Set up a symbolic link to allow for easy Python module imports. Then check to make sure the link works (it is a Unix link so check from shell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 3169,
     "status": "ok",
     "timestamp": 1601121478276,
     "user": {
      "displayName": "Thomas Goter",
      "photoUrl": "",
      "userId": "00883949598941594885"
     },
     "user_tz": 240
    },
    "id": "lnsWI7TngGOd",
    "outputId": "53e74cb7-9a99-45d0-fc76-c578d06e763f"
   },
   "outputs": [],
   "source": [
    "!ln -s './bigearthnet-models/' bemodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 3163,
     "status": "ok",
     "timestamp": 1601121478277,
     "user": {
      "displayName": "Thomas Goter",
      "photoUrl": "",
      "userId": "00883949598941594885"
     },
     "user_tz": 240
    },
    "id": "GGLMTMT_gyvz",
    "outputId": "9cd87b89-0dfc-4dd8-c201-9a89e6ae583a"
   },
   "outputs": [],
   "source": [
    "!ls bemodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIho5IV_giss"
   },
   "outputs": [],
   "source": [
    "from bemodels import tensorflow_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nK0uOlk8utUU"
   },
   "source": [
    "## Process All of the BigEarthNet data\n",
    "This simple script will loop over all of the subfolders in the BigEarthNet-v1.0 folder. Currently this folder does not contain the entirety of the BigEarthNet Dataset. Due to this issue, the original scripting was modified to run through the train, test, val sets and only process files if they exist. The previous script simply aborted if a file was listed in the train.csv file and was not in the directory.\n",
    "\n",
    "### Note: This processing takes a really long time. \n",
    "We need to determine if there is a better way to get this data ready for ingestion into our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "11N4a4EE-0HGNHlfvZEA3G8x3YzUjEkgn"
    },
    "id": "njMMPbVdfW_f",
    "outputId": "b9d26719-73d9-4a29-b160-76b70fae086e"
   },
   "outputs": [],
   "source": [
    "with open('./bigearthnet-models/label_indices.json', 'rb') as f:\n",
    "    label_indices = json.load(f)\n",
    "\n",
    "root_folder = big_earth_path\n",
    "out_folder = './tfrecords'\n",
    "splits = glob(f'./bigearthnet-models/splits/train.csv')\n",
    "\n",
    "# Checks the existence of patch folders and populate the list of patch folder paths\n",
    "folder_path_list = []\n",
    "if not os.path.exists(root_folder):\n",
    "    print('ERROR: folder', root_folder, 'does not exist')\n",
    "\n",
    "try:\n",
    "  patch_names_list = []\n",
    "  split_names = []\n",
    "  for csv_file in splits:\n",
    "    patch_names_list.append([])\n",
    "    split_names.append(os.path.basename(csv_file).split('.')[0])\n",
    "    with open(csv_file, 'r') as fp:\n",
    "      csv_reader = csv.reader(fp, delimiter=',')\n",
    "      for row in csv_reader:\n",
    "        patch_names_list[-1].append(row[0].strip())    \n",
    "except:\n",
    "    print('ERROR: some csv files either do not exist or have been corrupted')\n",
    "\n",
    "tensorflow_utils.prep_tf_record_files(\n",
    "    root_folder, out_folder, \n",
    "    split_names, patch_names_list, \n",
    "    label_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3o4Ya6iMz66"
   },
   "outputs": [],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(\"./tfrecords/full_test.tfrecord\")\n",
    "\n",
    "shards = 20\n",
    "\n",
    "for i in range(shards):\n",
    "    writer = tf.data.experimental.TFRecordWriter(f\"./tfrecords/test-part-{i}.tfrecord\")\n",
    "    writer.write(raw_dataset.shard(shards, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(\"./tfrecords/full_train.tfrecord\")\n",
    "\n",
    "shards = 50\n",
    "\n",
    "for i in range(shards):\n",
    "    writer = tf.data.experimental.TFRecordWriter(f\"./tfrecords/train-part-{i}.tfrecord\")\n",
    "    writer.write(raw_dataset.shard(shards, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(\"./tfrecords/full_val.tfrecord\")\n",
    "\n",
    "shards = 20\n",
    "\n",
    "for i in range(shards):\n",
    "    writer = tf.data.experimental.TFRecordWriter(f\"./tfrecords/val-part-{i}.tfrecord\")\n",
    "    writer.write(raw_dataset.shard(shards, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM/8/SHpGq20T8v/exkRAUj",
   "collapsed_sections": [],
   "name": "preprocess_tfrecords.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
