{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Z0HiNpYt6zD"
   },
   "source": [
    "# Big Earth Net Preprocessing\n",
    "## Irrigation Capstone Fall 2020\n",
    "### TP Goter\n",
    "\n",
    "This notebook is used to preprocess the GeoTiff files that contain the Sentinel-2 MSI data comprising the BigEarthNet dataset into dataframes.  We originally were using tfrecords, but after creating balanced datasets. wehave little enough data to make dataframe storage a reasonable solution. We will use the the same standardization routine as used by the root Big Earth Net data, but we will package the standardized/scaled data into a single dataframe with binary labels. It is based on the preprocessing scripts from the BigEarthNet repo, but has been updated to work in Colaboratory with Python3.7+ and TensorFlow 2.3.\n",
    "\n",
    "This version of the preprocessor is for specifically isolating the irrigated and non-irrigated examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4841,
     "status": "ok",
     "timestamp": 1601156548878,
     "user": {
      "displayName": "Thomas Goter",
      "photoUrl": "",
      "userId": "00883949598941594885"
     },
     "user_tz": 240
    },
    "id": "b0r4VAo9eRWa",
    "outputId": "0d884a87-7e43-470a-98f9-0312431f02ba"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import os\n",
    "#from matplotlib import pyplot as plt\n",
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "#from google.colab import drive\n",
    "#import seaborn as sns\n",
    "#from matplotlib.cm import get_cmap\n",
    "#import folium\n",
    "#import gdal\n",
    "import rasterio\n",
    "import csv\n",
    "import json\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4836,
     "status": "ok",
     "timestamp": 1601156548880,
     "user": {
      "displayName": "Thomas Goter",
      "photoUrl": "",
      "userId": "00883949598941594885"
     },
     "user_tz": 240
    },
    "id": "i8AXp5QRfCMR",
    "outputId": "58c67cd6-4e0a-452b-ba6b-2414f9a44ba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.2\n",
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(pd.__version__)\n",
    "print(tf.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFjjrUFAucPm"
   },
   "source": [
    "## Mount Google Drive and Set Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4830,
     "status": "ok",
     "timestamp": 1601156548880,
     "user": {
      "displayName": "Thomas Goter",
      "photoUrl": "",
      "userId": "00883949598941594885"
     },
     "user_tz": 240
    },
    "id": "Q5hp7SNtfCvY",
    "outputId": "3db919a6-5cd3-4949-8f0a-0f0e03e62145"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8nvKWoMlfGZ4"
   },
   "outputs": [],
   "source": [
    "#base_path = '/content/gdrive/My Drive/Capstone Project'\n",
    "big_earth_path ='./BigEarthNet-v1.0/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nK0uOlk8utUU"
   },
   "source": [
    "## Convert data to dataframes instead of TFRecords\n",
    "\n",
    "We already have our splits in csv files in the bigearthnet-models/splits folders. So we just need to read in these files, and concatenate them into one list. We can then convert that to a labeled dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = 'balanced_train_4'\n",
    "filenames_tif = list(pd.read_csv(f'./bigearthnet-models/splits/{FILE}.csv')['file'])\n",
    "filenames_tif = [f'{file}/{file.split(\"/\")[-1]}' for file in filenames_tif]\n",
    "filenames_tif[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('/'.join(filenames_tif[0].split('/')[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAND_NAMES = ['B01', 'B02', 'B03', 'B04', 'B05',\n",
    "              'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "\n",
    "BAND_STATS = {\n",
    "    'mean': {\n",
    "        'B01': 340.76769064,\n",
    "        'B02': 429.9430203,\n",
    "        'B03': 614.21682446,\n",
    "        'B04': 590.23569706,\n",
    "        'B05': 950.68368468,\n",
    "        'B06': 1792.46290469,\n",
    "        'B07': 2075.46795189,\n",
    "        'B08': 2218.94553375,\n",
    "        'B8A': 2266.46036911,\n",
    "        'B09': 2246.0605464,\n",
    "        'B11': 1594.42694882,\n",
    "        'B12': 1009.32729131\n",
    "    },\n",
    "    'std': {\n",
    "        'B01': 554.81258967,\n",
    "        'B02': 572.41639287,\n",
    "        'B03': 582.87945694,\n",
    "        'B04': 675.88746967,\n",
    "        'B05': 729.89827633,\n",
    "        'B06': 1096.01480586,\n",
    "        'B07': 1273.45393088,\n",
    "        'B08': 1365.45589904,\n",
    "        'B8A': 1356.13789355,\n",
    "        'B09': 1302.3292881,\n",
    "        'B11': 1079.19066363,\n",
    "        'B12': 818.86747235\n",
    "    }\n",
    "}\n",
    "\n",
    "# Use this one-liner to standardize each feature prior to reshaping.\n",
    "def standardize_feature(data, band_name):\n",
    "        return ((tf.dtypes.cast(data, tf.float32) - BAND_STATS['mean'][band_name]) / BAND_STATS['std'][band_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "std_data = []\n",
    "for file in tqdm(filenames_tif):\n",
    "    bands = {}\n",
    "    std_bands = {}\n",
    "    for band_name in BAND_NAMES:\n",
    "        # First finds related GeoTIFF path and reads values as an array\n",
    "        band_path = f'{file}_{band_name}.tif'\n",
    "        band_ds = rasterio.open(band_path)\n",
    "        bands[band_name] = np.array(band_ds.read(1))\n",
    "        std_bands[band_name] = standardize_feature(np.array(band_ds.read(1)), band_name)\n",
    "\n",
    "    #     print(f'B01 Shape: {bands[\"B01\"].shape}')\n",
    "    #     print(f'B02 Shape: {bands[\"B02\"].shape}')\n",
    "    #     print(f'B03 Shape: {bands[\"B03\"].shape}')\n",
    "    #     print(f'B04 Shape: {bands[\"B04\"].shape}')\n",
    "    #     print(f'B05 Shape: {bands[\"B05\"].shape}')\n",
    "    #     print(f'B06 Shape: {bands[\"B06\"].shape}')\n",
    "    #     print(f'B07 Shape: {bands[\"B07\"].shape}')\n",
    "    #     print(f'B08 Shape: {bands[\"B08\"].shape}')\n",
    "    #     print(f'B8A Shape: {bands[\"B8A\"].shape}')\n",
    "    #     print(f'B09 Shape: {bands[\"B09\"].shape}')\n",
    "    #     print(f'B11 Shape: {bands[\"B11\"].shape}')\n",
    "    #     print(f'B12 Shape: {bands[\"B12\"].shape}')\n",
    "    \n",
    "#     bands_10m = np.stack([bands['B04'],\n",
    "#                           bands['B03'],\n",
    "#                           bands['B02'],\n",
    "#                           bands['B08']], axis=2)\n",
    "\n",
    "#     bands_20m = np.stack([bands['B05'],\n",
    "#                           bands['B06'],\n",
    "#                           bands['B07'],\n",
    "#                           bands['B8A'],\n",
    "#                           bands['B11'],\n",
    "#                           bands['B12']], axis=2)\n",
    "    \n",
    "    std_bands_10m = np.stack([std_bands['B04'],\n",
    "                          std_bands['B03'],\n",
    "                          std_bands['B02'],\n",
    "                          std_bands['B08']], axis=2)\n",
    "\n",
    "    std_bands_20m = np.stack([std_bands['B05'],\n",
    "                          std_bands['B06'],\n",
    "                          std_bands['B07'],\n",
    "                          std_bands['B8A'],\n",
    "                          std_bands['B11'],\n",
    "                          std_bands['B12']], axis=2)\n",
    "    \n",
    "    \n",
    "#     msi_bands = np.concatenate([bands_10m, \n",
    "#                           cv2.resize(bands_20m, dsize=(120, 120), interpolation=cv2.INTER_CUBIC)],axis=2)\n",
    "    \n",
    "    msi_std_bands = np.concatenate([std_bands_10m, \n",
    "                          cv2.resize(std_bands_20m, dsize=(120, 120), interpolation=cv2.INTER_CUBIC)],axis=2)\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    file_json_path =  f'{file}_labels_metadata.json'\n",
    "   \n",
    "\n",
    "    with open(file_json_path, 'rb') as f:\n",
    "        patch_json = json.load(f)\n",
    "\n",
    "    if 'Permanently irrigated land' in patch_json['labels']:\n",
    "        label = np.array(1)\n",
    "    else:\n",
    "        label = np.array(0)\n",
    "        \n",
    "#     data.append((msi_bands, labels))\n",
    "    std_data.append((msi_std_bands, label))\n",
    "    \n",
    "# df = pd.DataFrame(data, columns=['X', 'y'])\n",
    "# del data\n",
    "\n",
    "std_df = pd.DataFrame(std_data, columns=['X', 'y'])\n",
    "del std_data\n",
    "\n",
    "std_df.to_pickle(f'./bigearthnet-models/splits/{FILE}.pkl')\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM/8/SHpGq20T8v/exkRAUj",
   "collapsed_sections": [],
   "name": "preprocess_tfrecords.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
